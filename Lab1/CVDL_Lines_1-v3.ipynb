{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1VCPf62X3wrqBt6xL9z5R8NpE74Soe_9B","timestamp":1602712365126}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PlysHko5nrhx"},"source":["# **MET Computer Vision with Deep Learning**\n","# **Lab1 - Line detection**\n","\n","2024 - Josep Ramon Morros - [GPI @ IDEAI](https://imatge.upc.edu/web/) Research group // [ETSETB – UPC.TelecosBCN](https://telecos.upc.edu/ca)\n"]},{"cell_type":"markdown","metadata":{"id":"NvUPuqUbyKmu"},"source":["---\n","\n","### Write here your name\n","\n","**Name**:\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"6_3_HDm6oD3Q"},"source":["### **Introduction**\n","\n","This practical work has two objectives: first, to analyze the operation of some low-level algorithms. Second, to use these tools to solve practical problems.\n","\n","As we will be using a deep learning method, we will configure Colab to use a GPU for faster operation:\n","\n","Runtime --> Change runtime type --> Hardware acceleration: GPU\n","\n","We will be using the some additional images and videos. First of all download the file cvdl_l1.zip from ATENEA and upload it to the Colaboratory notebook using the menu on the left (click the folder icon,  upload the cvdl_l1.zip file). **Wait until the upload is complete**, it may take a long time."]},{"cell_type":"markdown","metadata":{"id":"00L2UpJEs3uq"},"source":["\n","*Decompress* the zip file to use its contents:`"]},{"cell_type":"code","metadata":{"id":"37ecHsOey3FA"},"source":["!unzip cvdl_l1.zip\n","!rm cvdl_l1.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hoTADApSKtZK"},"source":["### **Pre-processing**\n","\n","The following video has been captured by a traffic surveillance camera."]},{"cell_type":"code","metadata":{"id":"LBhLbQI2K50l"},"source":["from IPython.display import HTML\n","from base64 import b64encode\n","\n","# Extracted from https://stackoverflow.com/questions/57377185/how-play-mp4-video-in-google-colab\n","mp4 = open('cvdl_l1/video/traffic_camera.mp4','rb').read()\n","data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","HTML(\"\"\"\n","<video width=800 controls>\n","      <source src=\"%s\" type=\"video/mp4\">\n","</video>\n","\"\"\" % data_url)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KYnDrrKPNnDE"},"source":["Let's define a useful function to display images:"]},{"cell_type":"code","metadata":{"id":"enLJNFL2CXgn"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy import ndimage as ndi\n","\n","from skimage import feature\n","from skimage import filters\n","from scipy.stats import mode\n","import cv2\n","\n","from cvdl_l1.code.display_images import display_image, display_images"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1629le1kIwV9"},"source":["\n","We are interested in detecting the lines that separate the road lanes. The problem here is that the passing cars make it difficult to see the lines. For this, we will try to obtain an image of the background (the road) without the cars. If we look at the color of a given pixel of the road along the video sequence, we will see that in the majority of the frames of the video sequance, the pixel will have the color of the road, and only in a few frames  the color of a car because the cars move very fast and the road is static.\n","\n","Now, we will average a large number of frames (for instance, 100) and we will examine the average image:"]},{"cell_type":"code","metadata":{"id":"jQx-NtVBL2Y4"},"source":["cap = cv2.VideoCapture('cvdl_l1/video/traffic_camera.mp4')\n","\n","ff = True\n","# Define a 3D matrix to store the 100 images\n","# The video is 640x360\n","\n","# Create an empy image of 360 rows and 640 columns, with 100 channels.\n","allfr = np.zeros((100,360,640))\n","\n","# Loop over frames from the video stream and accumulate\n","# all frames in a single variable\n","for ii in range(100):\n","\n","    if ii%10 == 0:\n","      print ('Processing frame {}'.format (ii))\n","    ret,frame = cap.read() # read the vieo frame\n","\n","    if ret is False: # If something failed ...\n","        break\n","\n","    # convert the frame to grayscale,\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","    # Store the frame in the matrix\n","    allfr[ii,:,:] = gray\n","\n","# Average the 100 frames.\n","avg = np.average(allfr,axis=0)\n","\n","# Convert to uint8 data type\n","avg = avg.astype(np.uint8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zqfqR9vy1h1D"},"source":["Let's display the result:"]},{"cell_type":"code","metadata":{"id":"ViDRpFuY1k6O"},"source":["display_image(avg, 'Average image', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0iW_syx3OPSs"},"source":["\n","We can also compute the [mode](https://en.wikipedia.org/wiki/Mode_(statistics)) of the 100 frames. This is, for each pixel, the gray level value that occurs more times along the 100 frames."]},{"cell_type":"code","metadata":{"id":"2Fpe2GgvOfUa"},"source":["print(avg.shape)\n","print(allfr.shape)\n","\n","mde  = mode(allfr,axis=0)[0].astype(np.uint8)\n","display_image(mde, 'Mode image', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6rKDgpAwPHY_"},"source":["Comment the results in both cases. Explain why the cars have dissapeared from the image.\n","\n","<font color=red>**Q1: Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"8ZgzPdypyuPJ"},"source":["Note that in the images are in fact matrices. Most Computer Vision libraries use numpy arrays to represent images. Let's examine image **avg**. We can know its dimensions. For this we will use the shape property of the array:"]},{"cell_type":"code","metadata":{"id":"gYnVCq0UzB0H"},"source":["mde.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4rnIyHtezyR7"},"source":["As we can see, the image is two dimensions (gray level). The number of rows is 360 and the number of columns is 640. We can access any pixel to know its value: or to change it:"]},{"cell_type":"code","metadata":{"id":"GWPQJClMz-Wf"},"source":["mde[0][0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m7LZZfbfAMVz"},"source":["mde[0][0] = 25\n","mde[0][0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p4p_3usS0tZT"},"source":["However ins most cases, it is more practical to use library functions to deal with images. OpenCV and scikit-image are examples of computer vision libraries. There are many others (bob, glib, etc.). In the previous examples we have used several of these functions: np.average(), cv2.imread(), etc."]},{"cell_type":"markdown","metadata":{"id":"NCpHD7-XoOMM"},"source":["### **Edge detection:**\n","\n","Perform edge detection on image **avg** using three different methods: thresholding of the gradient (Sobel), Canny, and a deep learning method, Holistically Nested Edge Detection [1].\n","\n","[1] Saining Xie, Zhuowen Tu, \"Holistically-Nested Edge Detection\", arXiv:1504.06375\n"]},{"cell_type":"markdown","metadata":{"id":"AS0xRVv8pws-"},"source":["\n","**Important note**: resizing (downsampling) the resulting contour images for visualization may affect the contour image contents. Some contours may be removed by the downsampling process. To avoid this always  visualize contour images at 100% (use size=1 in display_image) or downsample the images BEFORE contour detection."]},{"cell_type":"markdown","metadata":{"id":"f9C5e9M3hsf8"},"source":["Compute contours using a binarization of the gradient. Adjust the binarization threshold (threshold_sobel) so that the lane separation lines are correctly detected, with the minimum amount of 'unwanted' contours as possible. Unwanted contours are those that are not lane lines."]},{"cell_type":"code","metadata":{"id":"eoV2wr_T1rPj"},"source":["# Compute edges using sobel magnitude\n","edges = filters.sobel(avg)\n","\n","# Maximum value of the gradient image.\n","# The threshold should be between zero and this value\n","print (np.max(edges))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TFDHKZvUu-pJ"},"source":["In the next code cell, select the most approriate value for threshold_sobel, a threshold on the magnitude of the gradient at each point. Fill the parts marked with TODO. Please, leave the # TODO comments in the modified code."]},{"cell_type":"code","metadata":{"id":"r8Wvk1APAu8b"},"source":["# Threshold. Select the most approriate value for threshold_sobel, a threshold\n","# on the magnitude of the gradient at each point.\n","threshold_sobel = # TODO 1\n","ret, im_bw  = cv2.threshold(edges, threshold_sobel, 255, cv2.THRESH_BINARY)\n","display_image(edges, 'Gradient (Sobel)', size=1)\n","display_image(im_bw, 'Binarized gradient', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aAZJ1dB361eU"},"source":["Now, let's compute edges usign Canny with custom parameters. We will use different sets of values for sigma, low_threshold and high_threshold to demonstrate the effect of changing each of these values.\n","\n","Remember that increasing the value of sigma makes the contours weaker, so that you will have to lower the values of high_threshold and low_threshold accordingly.\n","\n","Finally, adjust the parameters so that the lane separation lines are correctly detected, with the minimum amount of 'unwanted' contours as possible, Unwanted contours are those that are not lane lines."]},{"cell_type":"code","metadata":{"id":"DvsQ-BKL7OKa"},"source":["# Compute edges using canny with automatic/default parameters\n","edges = feature.canny(avg)\n","display_image(edges, 'Canny (default)', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VrF6k2S6ti_V"},"source":["In the next code cell, try different values for the sigma parameter and explain the effect of increasing its value."]},{"cell_type":"code","metadata":{"id":"hmb9fdGGfyrY"},"source":["# Compute edges using canny with custom parameters. See the effect of increasing sigma.\n","sigma          = 2\n","low_threshold  = 25 # Default value\n","high_threshold = 51 # Default value\n","edges = feature.canny(avg, sigma=sigma, low_threshold=low_threshold, high_threshold=high_threshold)\n","display_image(edges, 'Canny (sigma)', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OCboRMSftwyY"},"source":["Comment the effect of incresing sigma.\n","\n","<font color=red>**Q2: Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"Q-EltimwuZAC"},"source":["In the next code cell, adjust thresholds to improve detection of lane lines"]},{"cell_type":"code","metadata":{"id":"Ei5H2XPH2jmu"},"source":["# Compute edges using canny with custom parameters. Adjust thresholds to improve detection of lane lines.\n","sigma          = 2\n","low_threshold  = # TODO 2\n","high_threshold = # TODO 2\n","edges = feature.canny(avg, sigma=sigma, low_threshold=low_threshold, high_threshold=high_threshold)\n","display_image(edges, 'Canny (sigma)', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vfx0dyFmuSig"},"source":["In the next code cell, try different values for the low_threshold parameter and comment the effect of setting a very low velue for low_threshold. Hint: look for contours that should not be present"]},{"cell_type":"code","metadata":{"id":"leO9KCbu2mZN"},"source":["# Comment the effect of setting a very low velue for low_threshold.\n","# Hint: look for contours that should not be present\n","sigma          = 2\n","low_threshold  = 6\n","high_threshold = 25\n","edges = feature.canny(avg, sigma=sigma, low_threshold=low_threshold, high_threshold=high_threshold)\n","display_image(edges, 'Canny (low_thr)', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9kyxU-BIuxAs"},"source":["Comment the effect of using a small value for low_threshold.\n","\n","<font color=red>**Q3: Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"wgPhgx1uvg3U"},"source":["In the next code cell, try different values for the high_threshold parameter and comment the effect of setting a very high value for low_threshold."]},{"cell_type":"code","metadata":{"id":"8gdYbYJU2pfK"},"source":["# Comment the effect of setting a very high velue for high_threshold.\n","sigma          = 2\n","low_threshold  = 15\n","high_threshold = 55\n","edges = feature.canny(avg, sigma=sigma, low_threshold=low_threshold, high_threshold=high_threshold)\n","display_image(edges, 'Canny (high_thr)', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CAHfIsDkvnrk"},"source":["Comment the effect of using a large value for high_threshold.\n","\n","<font color=red>**Q4: Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"N43Xh53Wv_fU"},"source":["In the next code cell, Select the best parameters you can obtain. Try to preserve lane lines and to remove other contours."]},{"cell_type":"code","metadata":{"id":"CsqzWCBofpWC"},"source":["# Select the best parameters you can find. Preserve lane lines and try to remove other contours.\n","sigma          = # TODO 3\n","low_threshold  = # TODO 3\n","high_threshold = # TODO 3\n","edges = feature.canny(avg, sigma=sigma, low_threshold=low_threshold, high_threshold=high_threshold)\n","display_image(edges, 'Canny (high_thr)', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HBal1Y_xEiMo"},"source":["Compute edges usign HED. HED does not provide a method for thinning the contours. The function thin_contours() implements a thinning method inspired in the Canny approach: Non-maxima suppression plus contour tracking using hysteresis. Select a good choice of values for low_threshold and high_threshold so that you are satisfied with the result (there is not a unique valid solution)."]},{"cell_type":"code","metadata":{"id":"2Wcs3--qEssU"},"source":["from cvdl_l1.hed.hed_detect import hed_detect\n","from cvdl_l1.code.contour_functions import thin_contours\n","\n","avg = cv2.cvtColor(avg,cv2.COLOR_GRAY2RGB)\n","edges6   = hed_detect(avg)\n","display_image(edges6, 'HED', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUthjA2NFAqf"},"source":["# Adjust parameters:\n","low_threshold4  = # TODO 4\n","high_threshold4 = # TODO 4\n","bw_image = thin_contours(edges6, low_threshold4, high_threshold4)\n","display_image(bw_image, 'HED (thin cont.)', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fwO7vOff8F00"},"source":["Compare the results for the three methods (gradient thresholding, Canny and HED) in terms of performance and simplicity of use. Comment the effects of the parameters on each method.\n","\n","<font color=red>**Q5: Answer**:  </font><br>\n","<font color=blue>\n"," (answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"rgNU3xviotyh"},"source":["### **Line detection with Hough**\n","We will use the Hough transform functions to extract the parameters of the lane lines. You should look at the [example](http://scikit-image.org/docs/dev/auto_examples/edges/plot_line_hough_transform.html) and to the [reference](https://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.hough_line) for more information on how the functions work.\n","\n","Now let's compute the lines parameters and display the **avg** image with these contours superimposed:"]},{"cell_type":"code","metadata":{"id":"a91imr7_NONQ"},"source":["from skimage.transform import (hough_line, hough_line_peaks)\n","\n","# Definition of the angles to test. We compute the HT in steps of 1º\n","tested_angles = np.linspace(-np.pi/2,np.pi/2,180)\n","\n","h, theta_vect, d = hough_line(edges, theta=tested_angles)\n","\n","# Set the threshold to discard lines with few points. The threshold\n","# is the number of contour points of each line\n","thr = 0.5*np.max(h) # This is the default value\n","\n","# Display the lines found:\n","out = avg.copy() # Create a version of the average image to display the lines\n","for _, theta, rho in zip(*hough_line_peaks(h, theta_vect, d, threshold=thr)):\n","    a = np.cos(theta)\n","    b = np.sin(theta)\n","    x0 = a*rho\n","    y0 = b*rho\n","    # Create two points of the line outside the image so that the line\n","    # traverses all the image\n","    x1 = int(x0 + 1000*(-b))\n","    y1 = int(y0 + 1000*(a))\n","    x2 = int(x0 - 1000*(-b))\n","    y2 = int(y0 - 1000*(a))\n","    cv2.line(out,(x1,y1),(x2,y2),(0,0,255),2)\n","\n","display_image(out, 'Detected lines', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PagRkzXNOwnS"},"source":["In the results image, we can see that many unwanted  lines are detected. This happens because the functions returns all peaks in the Hough space. To remove the unwanted lines, we have several options:"]},{"cell_type":"markdown","metadata":{"id":"srx4OlEdP7bH"},"source":["1. The first option is to set a higher threshold in the hough_line_peaks function. This will cause that only most important peaks, representing lines with many pixels, to be detected.\n","\n","> Select a threshold so that you recover as many lane lines as possible with the smallest amount of unwanted lines."]},{"cell_type":"code","metadata":{"id":"AJc4-XAHQdfZ"},"source":["# Definition of the angles to test. We compute the HT in steps of 1º\n","tested_angles = np.linspace(-np.pi/2,np.pi/2,180)\n","\n","h, theta_vect, d = hough_line(edges, theta=tested_angles)\n","\n","# Set the threshold to discard lines with few points. The threshold\n","# is the number of contour points of each line\n","thr = # TODO 5\n","\n","# Display the lines found:\n","out = avg.copy() # Create a version of the average image to display the lines\n","for _, theta, rho in zip(*hough_line_peaks(h, theta_vect, d, threshold=thr)):\n","    a  = np.cos(theta)\n","    b  = np.sin(theta)\n","    x0 = a*rho\n","    y0 = b*rho\n","    # Create two points of the line outside the image so that the line\n","    # traverses all the image\n","    x1 = int(x0 + 1000*(-b))\n","    y1 = int(y0 + 1000*(a))\n","    x2 = int(x0 - 1000*(-b))\n","    y2 = int(y0 - 1000*(a))\n","    cv2.line(out,(x1,y1),(x2,y2),(0,0,255),2)\n","\n","display_image(out, 'Detected lines', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bZTvxYN1PLNj"},"source":["2. Another solution in this case is to use a mask to define the region of interest. As the traffic camera is static and we are interested only in the information in the traffic lanes, we can construct a mask to remove the unwanted contours. The mask can be defined by hand, by looking at the **avg** image computed previously. Let's load and visualize a predefined mask:"]},{"cell_type":"code","metadata":{"id":"Tijhz4K9C3gI"},"source":["from skimage.transform import (hough_line, hough_line_peaks)\n","\n","# Load the image defining the mask\n","roi_mask = cv2.imread('cvdl_l1/image/traffic_camera_mask.png')\n","roi_mask = cv2.cvtColor(roi_mask, cv2.COLOR_BGR2GRAY)\n","\n","display_image(roi_mask*255, 'ROI mask', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_JezGtr-nEiC"},"source":[">Let's apply the ROI mask to the contour image. The\n","pixels of the regions of interest (the two lanes) have values 1. The pixels of the rest of the image have values 0. When multiplying pixelwise the contour image and the mask, the effect is to delete the regions outside the lanes."]},{"cell_type":"code","metadata":{"id":"PtOmeqwOm-r8"},"source":["edges = edges * roi_mask\n","display_image(edges, 'Simplified contours', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rdE01QvnRWfI"},"source":[">Now, we can repeat again the process. We expect that a lot of lines will not appear as the contour points that originated them have been removed. As previously, select the threshold to maximize results."]},{"cell_type":"code","metadata":{"id":"jPFE7dXTRi_9"},"source":["# Definition of the angles to test. We compute the HT in steps of 1º\n","tested_angles = np.linspace(-np.pi/2,np.pi/2,180)\n","\n","h, theta_vect, d = hough_line(edges, theta=tested_angles)\n","\n","# Set the threshold to discard lines with few points. The threshold\n","# is the number of contour points of each line\n","thr = # TODO 6\n","\n","# Display the lines found:\n","out = avg.copy() # Create a version of the average image to display the lines\n","for _, theta, rho in zip(*hough_line_peaks(h, theta_vect, d, threshold=thr)):\n","    a = np.cos(theta)\n","    b = np.sin(theta)\n","    x0 = a*rho\n","    y0 = b*rho\n","    # Create two points of the line outside the image so that the line\n","    # traverses all the image\n","    x1 = int(x0 + 1000*(-b))\n","    y1 = int(y0 + 1000*(a))\n","    x2 = int(x0 - 1000*(-b))\n","    y2 = int(y0 - 1000*(a))\n","    cv2.line(out,(x1,y1),(x2,y2),(0,0,255),2)\n","\n","display_image(out, 'Detected lines', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ScWJDarwV9fn"},"source":[">Depending on the selected threshold, yoy may find an unwanted line caused by the street lamp in the lower part of the image. The third method will make sure that this does not appear."]},{"cell_type":"markdown","metadata":{"id":"_G-l1iEJNRkV"},"source":["3. The last method consists on restrict the computation of the Hough transform to a certain angles. We can see that the lines of interest appear only in two triangular regions of the image (the regions of the left and right roads). As it can be seen in the next figure, the angles of the lane lines can not take all the values between $[0-360)$. They are restricted between two zones defined by, [$\\alpha, \\beta]$ and $[-\\beta,-\\alpha]$. Note that in scikit-image, the angles are defined between $-\\pi/2$ and $\\pi/2$"]},{"cell_type":"code","metadata":{"id":"mUuEjEkkqwD1"},"source":["# Load the image defining the angles\n","regions_angles = cv2.imread('cvdl_l1/image/angles.png')\n","display_image(regions_angles, 'Lanes definition ', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r5WjStUQrCx-"},"source":[">If we look at the Hough transform pseudocode:\n","```\n","\tfor each edge point (x, y) {\n","\t    // Compute parameters\n","\t\tfor (θ = 0; θ <= θmax; θ = θ+Δθ) {\n","\t\t\tρ = x·cos(θ)+y·sin(θ); // round off to integer (quantization)\n","\t\t\t(P[ρ][θ])++;\n","\t\t}\n","\t}\n","\n","```\n","# This is formatted as code\n","```\n","\n","\n","```\n",">we can see that the general solution checks all angles between 0 and $\\theta_{max}$, usually, [0 - 360). If we only want to detect lines with specific angles (in our case, the lines with angles compatible with the road lanes) we can modify the foor loop to take this into account. The functions for Hough lines in most Computer vision libraries already support this without having to reimplement the Hough transform. For instance, the hough_lines function in scikit-image:\n","```\n","skimage.transform.hough_line(image, theta=None)\n","```\n","has a parameter theta that is a list (vector) of all the angles to consider.\n","\n",">The process will consist of:\n","\n",">1. Detect contours on the image using one of the above methods (select the optimal parameters)\n","2. Simplify the contours by applying a Region Of Interest (ROI) mask\n","3. Create the theta vector with the values between $\\alpha$ to $\\beta$, $-\\beta$ to $-\\alpha$ in steps of np.pi/180 rad (1º)\n","4. Calculate the Hough transform\n","5. Select the optimal threshold in hough_line_peaks to detect the most important lines in the Hough space.\n","\n",">The $\\alpha$ and $\\beta$ angles are determined visually by looking at the image.\n","\n",">Write code to define the angles to test (tested_angles). Note that you will have to compose two angular spans. Define the threshold thr to display only lines between the lanes."]},{"cell_type":"code","metadata":{"id":"4yqRoMpk5Rl_"},"source":["# Definition of the angles to test\n","#tested_angles = np.linspace(-np.pi/2,np.pi/2,180)\n","tested_angles = # TODO 7\n","h, theta_vect, d = hough_line(edges, theta=tested_angles)\n","\n","# Set the threshold to discard lines with few points. The threshold\n","# is the number of contour points of each line\n","thr = # TODO 7\n","\n","# Display the lines found:\n","out = avg.copy() # Create a version of the average image to display the lines\n","for _, theta, rho in zip(*hough_line_peaks(h, theta_vect, d, threshold=thr)):\n","    a = np.cos(theta)\n","    b = np.sin(theta)\n","    x0 = a*rho\n","    y0 = b*rho\n","    # Create two points of the line outside the image so that the line\n","    # traverses all the image\n","    x1 = int(x0 + 1000*(-b))\n","    y1 = int(y0 + 1000*(a))\n","    x2 = int(x0 - 1000*(-b))\n","    y2 = int(y0 - 1000*(a))\n","    cv2.line(out,(x1,y1),(x2,y2),(0,0,255),2)\n","\n","display_image(out, 'Detected lines', size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OHPUQWHzG2rA"},"source":["Comment the results: Give  the values of $\\alpha$, $\\beta$, and thr. Explain briefly the results.\n","\n","<font color=red>**Q6: Answer**:  </font><br>\n","<font color=blue>\n","$\\alpha$ =\n","<br>\n","$\\beta$ =\n","<br>\n","thr =\n","<br>\n","<br>\n","(answer here)\n","</font>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IqVjQIfMdvD0"},"source":["### **Adjusting lines with RANSAC**\n","\n","HT1_DEMO.png and HT3_DEMO.png are two synthetic images that simulate the result from an edge or corner detection step. The white pixels in the images represent the contour points. Use RANSAC to obtain the parameters of the lines that best describe the set of points and the number of inliers in each image. Compare with the adjustment obtained directly with the least squares method.\n","\n","**NOTE**: Each point of the images is composed of multiple pixels to enhance visualization.  \n","\n"]},{"cell_type":"code","metadata":{"id":"dQrpEVM_1k4m"},"source":["from sklearn import linear_model\n","\n","# Noisy 'pseudo-contour' image\n","im1 = cv2.imread('cvdl_l1/image/HT1_DEMO.png')[:,:,0]\n","if im1 is None:\n","  print ('Could not read image')\n","\n","display_image(im1, 'Points image (with outliers)', size=4)\n","print (im1.shape)\n","\n","# Coordinates of the points in image\n","y1, X1 = np.where(im1 > 0)\n","\n","\n","# Code partially extracted from :\n","# https://scikit-learn.org/stable/auto_examples/linear_model/plot_ransac.html\n","\n","# Least Squares solution\n","lr = linear_model.LinearRegression()\n","lr.fit(X1.reshape(-1,1), y1.reshape(-1,1))\n","\n","# Fit linear model with RANSAC algorithm\n","ransac = linear_model.RANSACRegressor()\n","ransac.fit(X1.reshape(-1,1), y1.reshape(-1,1))\n","\n","# Get the inlier and outlier points\n","inlier_mask1  = ransac.inlier_mask_\n","outlier_mask1 = np.logical_not(inlier_mask1)\n","\n","# Predict data of estimated models\n","line_X1 = np.arange(X1.min(), X1.max())[:, np.newaxis]\n","line_y1 = lr.predict(line_X1)\n","line_y_ransac1 = ransac.predict(line_X1)\n","\n","# Compare estimated coefficients\n","print(\"Estimated coefficients (linear regression, RANSAC):\")\n","print('[{},{}], [{},{}]'.format(lr.coef_[0][0], lr.intercept_[0], ransac.estimator_.coef_[0][0],ransac.estimator_.intercept_[0]))\n","\n","# Plot the results\n","lw = 2\n","plt.scatter(X1[inlier_mask1], y1[inlier_mask1], color='yellowgreen', marker='.',\n","            label='Inliers')\n","plt.scatter(X1[outlier_mask1], y1[outlier_mask1], color='gold', marker='.',\n","            label='Outliers')\n","\n","plt.plot(line_X1, line_y1, color='navy', linewidth=lw, label='Linear regressor')\n","plt.plot(line_X1, line_y_ransac1, color='cornflowerblue', linewidth=lw,\n","         label='RANSAC regressor')\n","plt.legend(loc='lower right')\n","plt.xlabel(\"Input\")\n","plt.ylabel(\"Response\")\n","plt.gca().invert_yaxis()\n","plt.xlim([0,im1.shape[1]])\n","plt.ylim([im1.shape[1],0])\n","plt.gca().set_aspect('equal', adjustable='box')\n","\n","plt.show()\n","\n","\n","im2 = cv2.imread('cvdl_l1/image/HT3_DEMO.png')[:,:,0]\n","if im2 is None:\n","  print ('Could not read image')\n","\n","display_image(im2, 'Points image (no outliers)', size=4)\n","\n","# Coordinates of the points in image\n","y2, X2 = np.where(im2 > 0)\n","\n","# Least Squares solution\n","lr.fit(X2.reshape(-1,1), y2.reshape(-1,1))\n","# Fit linear model with RANSAC algorithm\n","ransac.fit(X2.reshape(-1,1), y2.reshape(-1,1))\n","\n","# Get the inlier and outlier points\n","inlier_mask2  = ransac.inlier_mask_\n","outlier_mask2 = np.logical_not(inlier_mask2)\n","\n","# Predict data of estimated models\n","line_X2 = np.arange(X2.min(), X2.max())[:, np.newaxis]\n","line_y2 = lr.predict(line_X2)\n","line_y_ransac2 = ransac.predict(line_X2)\n","\n","# Compare estimated coefficients\n","print(\"Estimated coefficients (linear regression, RANSAC):\")\n","print('[{},{}], [{},{}]'.format(lr.coef_[0][0], lr.intercept_[0], ransac.estimator_.coef_[0][0],ransac.estimator_.intercept_[0]))\n","\n","# Plot the results\n","lw = 2\n","plt.scatter(X2[inlier_mask2], y2[inlier_mask2], color='yellowgreen', marker='.',\n","            label='Inliers')\n","plt.scatter(X2[outlier_mask2], y2[outlier_mask2], color='gold', marker='.',\n","            label='Outliers')\n","plt.plot(line_X2, line_y2, color='navy', linewidth=lw, label='Linear regressor')\n","plt.plot(line_X2, line_y_ransac2, color='cornflowerblue', linewidth=lw,\n","         label='RANSAC regressor')\n","plt.legend(loc='lower right')\n","plt.xlabel(\"Input\")\n","plt.ylabel(\"Response\")\n","plt.gca().set_aspect('equal', adjustable='box')\n","plt.gca().invert_yaxis()\n","plt.xlim([0,im2.shape[1]])\n","plt.ylim([im2.shape[1],0])\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OFKwlPNK-zP7"},"source":["Comment the results of both images for the Least Squares approach and the RANSAC approach.\n","\n","<font color=red>**Q7: Answer**:  </font>\n","<br>\n","<font color=blue>\n"," (answer here)\n","</font>"]}]}